<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="utf-8">
    <title>ML FutureIsTech | Séparateurs à vaste marge (SVM)</title>
    <meta name="description" content="Jdu Sphinx Template by Julien Dubiel">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
	<script src="/static/html5shiv.js"></script>
	<script src="/static/respond.min.js"></script>
	<![endif]-->
    <script src="https://use.fontawesome.com/f82db3f45d.js"></script>
    <script type="text/javascript" async defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" media="screen">
	<link rel="stylesheet" type="text/css" href="_static/bootswatch.custom.min.css">
    <link rel="stylesheet" type="text/css" href="_static/simple-sidebar.css">

    <!-- css_files -->

    <!-- extrahead -->
	    <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
		<link rel="shortcut icon" href="_static/favicon.ico">
		<link rel="stylesheet" href="_static/custom.css">
    	<!-- <link rel="stylesheet" href="_static/basic.css" type="text/css" /> -->
    	<link rel="stylesheet" href="_static/pygments.css" type="text/css" />
	
    <link rel="shortcut icon" href="_static/favicon.ico"/>
	    <link rel="index" title="Index" href="genindex.html" />
	    <link rel="search" title="Recherche" href="search.html" />
	    <link rel="next" title="Réseaux de neuronnes" href="nn.html" />
	    <link rel="prev" title="Régression Linéaire" href="modeles_lineaires/regression_lineaire.html" />
</head><body>
<!--     <div class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <a href="https://juliendubiel.net/" class="navbar-brand">ML FutureIsTech</a>
                <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>
            <div class="navbar-collapse collapse" id="navbar-main">
                <ul class="nav navbar-nav"></ul>                
                <ul class="nav navbar-nav navbar-right"></ul>
            </div>
        </div>
    </div> -->
    <div id="wrapper">
        <div id="sidebar">
            <ul class="sidebar-nav nav-pills nav-stacked" id="menu">
	            <li class="clearafter df">
	            	<span class="al maz fa-stack fa-lg menu-toggle"><i class="fa fa-bars fa-stack-1x "></i></span>
	            	<a href="index.html" class="menu-title ar maz">ML FutureIsTech</a>
	            </li>
                <li class="clearafter df">
                	<span class="al maz fa-stack fa-lg menu-toggle-open"><i class="fa fa-search fa-stack-1x "></i></span>
                	<input type="text" class="ar maz form-control" style="width: 221px;" id="inputEmail" placeholder="Search a term">
                </li>
                
        <li class="menu-toggle-open">
            <a class="deep0" href="#">
                <div class="al deep0-number">1</div>
                <div class="ar deep0-title">Fiches</div>
            </a>
            <ul class="deep1">
                <li><a href="/fiches/vocabulaire.html">Vocabulaire</a></li><li><a href="/fiches/notations.html">Notations</a></li>
            </ul>
        </li>
        
        <li class="menu-toggle-open">
            <a class="deep0" href="#">
                <div class="al deep0-number">2</div>
                <div class="ar deep0-title">Les Bases</div>
            </a>
            <ul class="deep1">
                <li><a href="/les_bases/probabilites_et_statistiques.html">Probabilités et Statistiques</a></li><li><a href="/les_bases/calcul_matriciel.html">Calcul matriciel</a></li><li><a href="/les_bases/algebre_lineaire.html">Algèbre Linéaire</a></li><li><a href="/les_bases/geometrie_vectorielle.html">Géométrie vectorielle</a></li>
            </ul>
        </li>
        
        <li class="menu-toggle-open">
            <a class="deep0" href="#">
                <div class="al deep0-number">3</div>
                <div class="ar deep0-title">Methodologie</div>
            </a>
            <ul class="deep1">
                <li><a href="/methodologie/conception_d_un_modele.html">Conception d’un modèle</a></li><li><a href="/methodologie/mesurer_la_qualite_d_un_modele.html">Mesurer la qualité d’un modèle</a></li><li><a href="/methodologie/optimisation_hyperparametres.html">Optimisation des hyperparamètres d’un modèle</a></li>
            </ul>
        </li>
        
        <li class="menu-toggle-open">
            <a class="deep0" href="#">
                <div class="al deep0-number">4</div>
                <div class="ar deep0-title">Modèles Linéaires</div>
            </a>
            <ul class="deep1">
                <li><a href="/modeles_lineaires/regression_lineaire.html">Régression Linéaire</a></li>
            </ul>
        </li>
        
        <li class="menu-toggle-open">
            <a class="deep0" href="/svm.html">
                <div class="al deep0-number">5</div>
                <div class="ar deep0-title">Séparateurs à vaste marge (SVM)</div>
            </a>
        </li>
        
        <li class="menu-toggle-open">
            <a class="deep0" href="#">
                <div class="al deep0-number">6</div>
                <div class="ar deep0-title">Réseaux de neuronnes</div>
            </a>
            <ul class="deep1">
                <li><a href="/nn/mlp.html">Perceptron Multicouche (MLP)</a></li><li><a href="/nn/dnn.html">Deep Neural network (DNN)</a></li><li><a href="/nn/cnn.html">Convolutional Neural Network (CNN)</a></li>
            </ul>
        </li>
        
        <li class="menu-toggle-open">
            <a class="deep0" href="#">
                <div class="al deep0-number">7</div>
                <div class="ar deep0-title">Outils</div>
            </a>
            <ul class="deep1">
                <li><a href="/outils/spark.html">Spark</a></li><li><a href="/outils/cuda.html">Nvidia CUDA</a></li>
            </ul>
        </li>
        
        <li class="menu-toggle-open">
            <a class="deep0" href="/contribuer.html">
                <div class="al deep0-number">8</div>
                <div class="ar deep0-title">Contribuer</div>
            </a>
        </li>
        <!-- 
                <li class="menu-toggle-open">
                    <a class="deep0" href="#">
                        <div class="al deep0-number">1</div>
                        <div class="ar deep0-title">Fiches</div>
                    </a>
                    <ul class="deep1">
                        <li><a href="/fiches/vocabulaire.html">Vocabulaire</a></li>
                        <li><a href="/fiches/notations.html">Notations</a></li>
                    </ul>
                </li>
                <li class="menu-toggle-open">
                    <a class="deep0" href="#">
                        <div class="al deep0-number">2</div>
                        <div class="ar deep0-title">Les Bases</div>
                    </a>
                    <ul class="deep1">
                        <li><a href="les_bases/probabilites_et_statistiques.html">Probabilités et Statistiques</a></li>
                        <li><a href="/les_bases/algebre_lineaire.rst">Algèbre Linéaire</a></li>
                        <li><a href="/les_bases/geometrie_vectorielle.html">Géométrie vectorielle</a></li>
                    </ul>
                </li>
                <li class="menu-toggle-open">
				    <a class="deep0" href="#">
				    	<div class="al deep0-number">3</div>
				    	<div class="ar deep0-title">Méthodologie</div>
				    </a>
                    <ul class="deep1">
                        <li><a href="/methodologie/conception_d_un_modele.html">Conception d'un modèle</a></li>
                        <li><a href="/methodologie/mesurer_la_qualite_d_un_modele.html">Mesurer la qualité d'un modèle</a></li>
                    </ul>
                </li>
                <li class="menu-toggle-open">
				    <a class="deep0" href="#">
				    	<div class="al deep0-number">4</div>
				    	<div class="ar deep0-title">Modèles Linéaires</div>
				    </a>
                    <ul class="deep1">
                        <li><a href="/modeles_lineaires/regression_lineaire.html">Régression linéaire</a></li>
                    </ul>
                </li>
                <li class="menu-toggle-open">
                    <a class="deep0" href="#">
                        <div class="al deep0-number">5</div>
                        <div class="ar deep0-title">NN</div>
                    </a>
                    <ul class="deep1">
                        <li><a href="/nn/mlp.html">Perceptron Multicouche (MLP)</a></li>
                        <li><a href="/nn/cnn.html">Convolutional Neural Network (CNN)</a></li>
                        <li><a href="/nn/dnn.html">Deep Neural network (DNN)</a></li>
                    </ul>
                </li>
                <li class="menu-toggle-open">
                    <a class="deep0" href="#">
                        <div class="al deep0-number">6</div>
                        <div class="ar deep0-title" href="/svm.html">SVM</div>
                    </a>
                </li>
                <li class="menu-toggle-open">
                    <a class="deep0" href="#">
                        <div class="al deep0-number">7</div>
                        <div class="ar deep0-title">Outils</div>
                    </a>
                    <ul class="deep1">
                        <li><a href="/outils/spark.html">Spark</a></li>
                        <li><a href="/outils/cuda.html">Nvidia CUDA</a></li>
                    </ul>
                </li>
                <li class="menu-toggle-open">
				    <a class="deep0" href="#">
				    	<div class="al deep0-number">8</div>
				    	<div class="ar deep0-title">Contribuer</div>
				    </a>
                </li> -->
            </ul>
        </div>
        <!-- /#sidebar-wrapper -->
        <!-- Page Content -->
        <div id="page-content-wrapper">
		    <div class="container main-contents">
		        <div class="page-header" id="banner">
		            <h1>Séparateurs à vaste marge (SVM)</h1>
		        </div>

			    <ul class="breadcrumb">
		      		<li><a href="index.html">Home</a></li>
			        
		      		<li class="active">Séparateurs à vaste marge (SVM)</li>
			    </ul>

			    <div class="bs-docs-section clearfix article" id="article">
		            
  <div class="math">
\[\definecolor{red}{RGB}{255,0,0}
\definecolor{purple}{RGB}{154,0,154}
\definecolor{blue}{RGB}{0,0,224}
\definecolor{green}{RGB}{0,187,0}
\definecolor{yellow}{RGB}{255,234,0}
\definecolor{pink}{RGB}{255,77,166}
\definecolor{orange}{RGB}{255,120,0}
\definecolor{grey}{RGB}{128,128,128}
\definecolor{black}{RGB}{0,0,0}
\definecolor{white}{RGB}{255,255,255}
\newcommand{\R}{\_mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\P}{\mathbb{P}}
\newcommand\given[1][]{\:#1\vert\:}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Cov}{Cov}\]</div>
<div class="section" id="separateurs-a-vaste-marge-svm">
<h1>Séparateurs à vaste marge (SVM)<a class="headerlink" href="#separateurs-a-vaste-marge-svm" title="Lien permanent vers ce titre">¶</a></h1>
<div class="section" id="objectif">
<h2>Objectif<a class="headerlink" href="#objectif" title="Lien permanent vers ce titre">¶</a></h2>
<p>Optimiser une fonction représentant la meilleure droite (ou hyperplan) de séparation/frontière entre chaque classe. Il peut y avoir plusieurs frontières.
Maximiser la marge (distance) entre la frontière et les données :</p>
<img alt="_images/svm_1.jpg" src="_images/svm_1.jpg" />
<p>L’objectif du SVM est de trouver une dimension dans laquelle on peut linéairement séparer nos données. Il s’agit donc de trouver une fonction <span class="math">\(\phi\)</span> qui projette nos données dans une autre dimension :</p>
<div class="math">
\[\text{Données} \xrightarrow{\phi} \text{Caractéristiques}\]</div>
<div class="math">
\[X_n \xrightarrow{\phi} Y_m\]</div>
<img alt="_images/svm_2.png" src="_images/svm_2.png" />
<p>D’après le <a class="reference external" href="https://en.wikipedia.org/wiki/Cover's_theorem">théorème de Cover</a>, quel que soit le problème, il existe une dimension dans lequel il est linéairement séparable.</p>
<p>Le SVM est un problème convexe. On peut donc le résoudre avec une simple descente de gradient afin de trouver l’optimum global de notre problème. On peut aussi utiliser la méthode de Newton mais elle risque de prendre plus de temps.</p>
</div>
<div class="section" id="vecteur-support">
<h2>Vecteur support<a class="headerlink" href="#vecteur-support" title="Lien permanent vers ce titre">¶</a></h2>
<ul class="simple">
<li>Seul les données proches de la frontière sont utiles pour entrainer un SVM : ce sont les vecteurs support.</li>
<li>Les données éloignées de la frontière ne sont pas prises en compte pour l’entrainement.</li>
<li>Les vecteurs support sont donc ceux qui servent à définir la formule de la droite.</li>
<li>Nous cherchons à avoir un minimum de vecteurs support à la fin de l’entrainement.</li>
<li>Pour choisir entre deux kernels présentant des résultats similaires, il faut regarder le nombre de vecteurs support qu’ils utilisent. Celui qui en a le moins sera celui qui généralisera le mieux.</li>
<li>Dans nos problèmes d’optimisation, il s’agit de la variable <span class="math">\(\alpha\)</span>. Ainsi :<ul>
<li>Si le point est un vecteur support : <span class="math">\(\alpha &gt; 0\)</span></li>
<li>Si le point n’est pas un vecteur support : <span class="math">\(\alpha = 0\)</span></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="hard-margin">
<h2>Hard margin<a class="headerlink" href="#hard-margin" title="Lien permanent vers ce titre">¶</a></h2>
<p>C’est la 1ère version de l’algorithme qui consiste simplement à séparer deux ensembles distincts n’ayant pas de « bruit » avec un SVM linéaire. Attention, la notion de « bruit » n’est pas forcément correcte, on parlera plutôt de « cas particuliers », en effet, il peut arriver que ces exemples soient corrects et non pas issus d’un quelconque bruit.
Le problème d’optimisation associé dit problème « primal » est le suivant :</p>
<div class="math">
\[g(x) = w^{T}x + b\]</div>
<div class="math">
\[\arg\max_{w}\frac{2}{\left \|w\right \|}\leftrightarrow \arg\min_{w}\frac{1}{2}\left \|w\right \|^{2}\]</div>
<div class="math">
\[s.c.: \left\{ \begin{array}{ll} y_i(w^Tx_i+b)&gt;=1 \end{array} \right.\]</div>
<p>Cependant, dans certains cas, ce problème « primal » est trop complexe à résoudre.</p>
<p>On peut reformuler ce problème avec les <a class="reference external" href="https://fr.wikipedia.org/wiki/Multiplicateur_de_Lagrange">Multiplicateurs de Lagrange</a> (<span class="math">\(= \alpha_i\)</span>), c’est la version « dual » du problème :</p>
<div class="math">
\[\mathcal{L}(\alpha)=Q(\alpha)=\sum_{i}{\alpha_i}-\frac{1}{2}\sum_{i}{\sum_{j}{\alpha_i\alpha_jy_iy_j&lt;x_i,x_j&gt; }}\]</div>
<div class="math">
\[\begin{split}s.c.: \left\{ \begin{array}{ll} 0\leq \alpha_i\\\sum\limits_{i=1}^{N}{\alpha_iy_i}=0 \end{array} \right.\end{split}\]</div>
<p>Il ne nous reste donc plus qu’à trouver le <span class="math">\(\alpha=(\alpha_1 ... \alpha_n) \neq 0\)</span> qui maximise la marge. Un algorithme très utilisé pour résoudre ce problème d’optimisation quadratique est le « Sequential minimal optimization » ou « <a class="reference external" href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization">SMO</a> ».</p>
</div>
<div class="section" id="soft-margin">
<h2>Soft margin<a class="headerlink" href="#soft-margin" title="Lien permanent vers ce titre">¶</a></h2>
<p>C’est la 2nde version de l’algorithme, qui prend en compte le bruit. Sans prendre en compte le bruit, le SVM serait forcé de trouver un séparateur linéaire parfait (qui classifie correctement tous les exemples), ce qui entrainerait surement un sur-apprentissage.</p>
<p>On introduit pour chaque exemple , ce qu’on appelle des « variables latentes » notées : <span class="math">\(\xi_i\geq 0\)</span>.</p>
<p>Les variables latentes …</p>
<ul class="simple">
<li>… <span class="math">\(\xi_i=0\)</span> indiquent que les exemples associés sont bien classés et se trouvent sur ou en dehors de la marge.</li>
<li>… <span class="math">\(0\leq \xi_i\leq 1\)</span> indiquent que les exemples associés se trouvent dans la marge de leur classe.</li>
<li>… <span class="math">\(1&lt; \xi_i\leq 2\)</span> indiquent que les exemples associés se trouvent dans la marge de l’autre classe.</li>
<li>… <span class="math">\(2&lt;\xi_i\)</span> indiquent que les exemples associés sont mal classés et en dehors de la marge.</li>
</ul>
<p>Celles-ci représentent en fait, pour chaque exemple dans la marge ou mal classé, la distance qui le sépare de la marge de sa classe.
On introduit pour la suite un hyperparamètre/une constante <a class="reference external" href="http://www.svms.org/parameters/">C</a> : qui contrôlera le compromis entre l’erreur (<span class="math">\(\textstyle\sum_{i}{\xi_i}\)</span>) et la marge.</p>
<ul class="simple">
<li>Lorsque <span class="math">\(C\)</span> est petit, on est « laxiste » :<ul>
<li>Faible variance : moindre sensibilité aux variations dans les exemples.</li>
<li>Biais plus fort : on veut des hypothèses de grande marge.</li>
</ul>
</li>
<li>Lorsque <span class="math">\(C\)</span> est grand, on est « sévère » :<ul>
<li>Forte variance : grande sensibilité aux particularités dans les exemples.</li>
<li>Biais plus faible : on est moins exigeant sur la marge</li>
<li>Avec <span class="math">\(C\rightarrow +\infty\)</span>, on va tendre vers un SVM « hard margin » (on fait du sur-apprentissage).</li>
</ul>
</li>
</ul>
<p>On peut reformuler ce problème avec les Multiplicateurs de Lagrange (<span class="math">\(=\alpha_i\)</span>), c’est la version « Dual » du problème :</p>
<div class="math">
\[\arg\min_{w}\frac{1}{2}\left \|w\right \|^{2}+C\sum_{i}{\xi_i}\]</div>
<div class="math">
\[\begin{split}s.c.: \left\{ \begin{array}{ll} 0\leq \alpha_i\leq C \\\sum\limits_{i=1}^{N}{\alpha_iy_i}=0 \end{array} \right.\end{split}\]</div>
<p>Il ne nous reste donc plus qu’à trouver le <span class="math">\(\alpha=(\alpha_1 ... \alpha_n) \neq 0\)</span> qui maximise la marge. On peut toujours utiliser l’algorithme « SMO ».</p>
</div>
<div class="section" id="kernel">
<h2>Kernel<a class="headerlink" href="#kernel" title="Lien permanent vers ce titre">¶</a></h2>
<p>Il peut être difficile de trouver une fonction qui projette les données dans la bonne dimension. Pour résoudre ce problème, les SVM introduisent des fonction dites « kernel ».
Il s’agit en fait simplement d’un produit scalaire :</p>
<div class="math">
\[k(a,b)=&lt;a,b&gt;=\phi(a)\cdot \phi(b)\]</div>
<p>D’après le théorème de <a class="reference external" href="https://en.wikipedia.org/wiki/Mercer%27s_theorem">Mercer</a>, <strong>toute fonction kernel continue, symétrique, semi-définie positive, peut être exprimée sous la forme d’un produit scalaire dans un espace de grande dimension</strong>.
Ainsi on travaille uniquement sur des produits scalaires, et non pas sur les valeurs des vecteurs. Cela permet d’avoir une dimension de projection infinie si nécessaire.</p>
<div class="section" id="proprietes">
<h3>Propriétés<a class="headerlink" href="#proprietes" title="Lien permanent vers ce titre">¶</a></h3>
<div class="math">
\[k(a,b)=k_1(a,b)+k_2(a,b)\]</div>
<div class="math">
\[k(a,b)=k_3(a,b)*k_4(a,b)\]</div>
<div class="math">
\[k(a,b)=c*k_5(a,b)\]</div>
<div class="math">
\[k(a,b)=k_6(\phi(a),\phi(b))\]</div>
</div>
<div class="section" id="fonctions-kernels-communes">
<h3>Fonctions kernels communes<a class="headerlink" href="#fonctions-kernels-communes" title="Lien permanent vers ce titre">¶</a></h3>
<ul class="simple">
<li>Kernel linéaire :</li>
</ul>
<div class="math">
\[k(x_i,x_j)=x_i^Tx_j+1\]</div>
<ul class="simple">
<li>Kernel polynomial d’ordre n :</li>
</ul>
<div class="math">
\[k(x_i,x_j)=(x_i^Tx_j+1)^n\]</div>
<ul class="simple">
<li>Kernel Laplacien :</li>
</ul>
<div class="math">
\[k(x_i,x_j)=\exp\left(-\frac{\left \| x_i-x_j \right \|}{\sigma}\right)\]</div>
<ul class="simple">
<li>Kernel exponentiel :</li>
</ul>
<div class="math">
\[k(x_i,x_j)=\exp\left(-\frac{\left \| x_i-x_j \right \|}{2\sigma^2} \right )\]</div>
<ul class="simple">
<li>Kernel Gaussien ou <a class="reference external" href="https://en.wikipedia.org/wiki/Radial_basis_function">RBF</a> (Radial Basis Function) :<ul>
<li>Plus <span class="math">\(\theta\)</span> est petit, plus il fait du sur-apprentissage</li>
<li>Plus <span class="math">\(\theta\)</span> est grand, plus il tend vers du linéaire</li>
</ul>
</li>
</ul>
<div class="math">
\[k(x_i,x_j)=\exp\left( \frac{\left \| x_i-x_j \right \|^2}{2\sigma^2} \right )=\exp\left(-\gamma\left \| x_i-x_j \right \|^2 \right )\]</div>
</div>
<div class="section" id="la-matrice-de-gram">
<h3>La matrice de Gram<a class="headerlink" href="#la-matrice-de-gram" title="Lien permanent vers ce titre">¶</a></h3>
<div class="math">
\[\begin{split}G=\begin{bmatrix} k(x_1,x_1) &amp; \dots &amp; k(x_1,x_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(x_n,x_1) &amp; \dots &amp; k(x_n,x_n) \end{bmatrix}=X^TX\end{split}\]</div>
<p>Si <span class="math">\(G\)</span> est semi-définie positive, alors <span class="math">\(\phi\)</span> existe. Équivalent à vérifier que <span class="math">\(G_i_j&gt;0\)</span>.</p>
</div>
<div class="section" id="exemple">
<h3>Exemple<a class="headerlink" href="#exemple" title="Lien permanent vers ce titre">¶</a></h3>
<p>Par exemple, on peut résoudre le problème de classification suivant :</p>
<img alt="_images/svm_3.png" src="_images/svm_3.png" />
</div>
<div class="section" id="astuces">
<h3>Astuces<a class="headerlink" href="#astuces" title="Lien permanent vers ce titre">¶</a></h3>
<ul class="simple">
<li>Toujours commencer par un kernel linéaire pour savoir si le problème est linéairement séparable dans la dimension actuelle.</li>
<li>Pour que les calculs soient rapides, il faut centrer-réduire les données.</li>
</ul>
</div>
</div>
</div>


                    
                    
		        </div>
		    </div>
		</div>
	</div>

	<script type="text/javascript">
	  var DOCUMENTATION_OPTIONS = {
	    URL_ROOT:    './',
	    VERSION:     '',
	    COLLAPSE_INDEX: false,
	    FILE_SUFFIX: '.html',
	    HAS_SOURCE:  true,
	    SOURCELINK_SUFFIX: '.txt'
	  };
	</script>

	<script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
	<script src="_static/custom.js"></script>

    <script>
	    $(".menu-toggle").click(function(e) {
	        if ($("#wrapper").hasClass("toggled")) {
		        $("#wrapper").toggleClass("toggled");
				$(".current").toggleClass("opened");
			} else {
		        $("#wrapper").toggleClass("toggled");
				$(".opened").toggleClass("opened");				
			}
	    });

	    function openUL(item) {
	    	var checkElement = item.next();
			if(checkElement.is('ul')) {
				if (checkElement.is(':visible')) {
	    			console.log("visible");
					checkElement.parent().toggleClass("opened");
				} else {
	    			console.log("unvisible");
					$(".opened").toggleClass("opened");
					checkElement.parent().toggleClass("opened");
				}
			}
	    };
	    $(".menu-toggle-open").click(function(e) {
	    	console.log(e);
	        if ($("#wrapper").hasClass("toggled")) {
	        	$("#wrapper").removeClass("toggled");
				$(".opened").toggleClass("opened");
				openUL($(this).find("a"));
	        }
	    });

	    $('#menu li a').click(function() {openUL($(this))});
    </script>

    <!-- script_files -->
		<script defer type="text/javascript" src="_static/jquery.js"></script>
		<script defer type="text/javascript" src="_static/underscore.js"></script>
		<script defer type="text/javascript" src="_static/doctools.js"></script>
		<script defer type="text/javascript" src="_static/translations.js"></script>
		<script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_RGBorMML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: {
                extensions: ["color.js"]
            }
        });
    </script>

</body>


 